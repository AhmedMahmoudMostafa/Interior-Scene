{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-06-17T10:57:55.105577Z","iopub.status.busy":"2024-06-17T10:57:55.104723Z","iopub.status.idle":"2024-06-17T10:57:59.612343Z","shell.execute_reply":"2024-06-17T10:57:59.611577Z","shell.execute_reply.started":"2024-06-17T10:57:55.105543Z"},"papermill":{"duration":6.300438,"end_time":"2024-04-26T06:27:38.860628","exception":false,"start_time":"2024-04-26T06:27:32.560190","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torchvision\n","from torchvision import datasets, models, transforms\n","import numpy as np\n","import os\n","import torch\n","from torch.utils.data import DataLoader, TensorDataset\n","from torchvision import transforms\n","import json\n","from PIL import Image\n","from __future__ import print_function, division\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.autograd import Variable\n","import numpy as np\n","import torchvision\n","from torchvision import datasets, models, transforms\n","import matplotlib.pyplot as plt\n","import time\n","import copy\n","import os\n","from PIL import ImageFile\n","ImageFile.LOAD_TRUNCATED_IMAGES = True\n"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.007303,"end_time":"2024-04-26T06:27:38.875710","exception":false,"start_time":"2024-04-26T06:27:38.868407","status":"completed"},"tags":[]},"source":["# **Read training data**"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-06-16T11:08:26.844806Z","iopub.status.busy":"2024-06-16T11:08:26.844058Z","iopub.status.idle":"2024-06-16T11:08:26.850943Z","shell.execute_reply":"2024-06-16T11:08:26.849931Z","shell.execute_reply.started":"2024-06-16T11:08:26.844770Z"},"trusted":true},"outputs":[],"source":["label_counts={ \"living room\" : 0,\n","                    \"kitchen\" : 0 ,\n","                    \"bedroom\" : 0,\n","                    \"bathroom\":0,\n","                    \"dining room\" : 0}"]},{"cell_type":"code","execution_count":44,"metadata":{"execution":{"iopub.execute_input":"2024-06-17T12:52:01.940445Z","iopub.status.busy":"2024-06-17T12:52:01.939543Z","iopub.status.idle":"2024-06-17T12:53:19.438458Z","shell.execute_reply":"2024-06-17T12:53:19.437382Z","shell.execute_reply.started":"2024-06-17T12:52:01.940408Z"},"papermill":{"duration":86.155179,"end_time":"2024-04-26T06:29:05.038035","exception":false,"start_time":"2024-04-26T06:27:38.882856","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["6370\n","6370\n"]}],"source":["import os\n","import json\n","from PIL import Image\n","import torchvision.transforms as transforms\n","\n","# Load the JSON file containing image paths\n","json_file_path = \"/kaggle/input/bbbbbbbbbbbbbbbbbbbbbb/test.json\"\n","with open(json_file_path, 'r') as f:\n","    data = json.load(f)\n","\n","# Load all images into memory and preprocess them\n","transform = transforms.Compose([\n","    transforms.Resize(224),\n","    transforms.CenterCrop(224),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","])\n","\n","Train_images = []\n","Train_labels1 = []\n","label_counts=0\n","\n","# Dictionary to keep track of image counts for each label\n","#label_counts = {\"Bathroom\": 0, \"Bedroom\": 0, \"Dining Room\": 0, \"Kitchen\": 0, \"Living Room\": 0}\n","\n","for image_info in data:\n","    image_path = os.path.join('/kaggle/input/data-set/data_set/', image_info['image_path'])\n","    t1 = image_info['label2']\n","    image_path = image_path.replace('\\\\', '/')\n","    \n","    if os.path.exists(image_path):\n","        image = Image.open(image_path).convert('RGB')\n","        image = transform(image)\n","        Train_images.append(image)\n","        Train_labels1.append(t1)\n","        \n","        # Increment the count for the current label\n","    label_counts += 1\n","        \n","#     # Check if we have read 2000 images for each label\n","#     if all(count >= 1000 for count in label_counts.values()):\n","#         break\n","\n","        \n","        \n","print(len(Train_images))\n","print(len(Train_labels1))"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.007842,"end_time":"2024-04-26T06:29:05.053374","exception":false,"start_time":"2024-04-26T06:29:05.045532","status":"completed"},"tags":[]},"source":["# **Read testing data**"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-06-16T11:12:36.355132Z","iopub.status.busy":"2024-06-16T11:12:36.354776Z","iopub.status.idle":"2024-06-16T11:12:36.360086Z","shell.execute_reply":"2024-06-16T11:12:36.359238Z","shell.execute_reply.started":"2024-06-16T11:12:36.355104Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["8998\n","8998\n"]}],"source":["print(len(Train_images))\n","print(len(Train_labels1))"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-06-17T10:58:02.708541Z","iopub.status.busy":"2024-06-17T10:58:02.708053Z","iopub.status.idle":"2024-06-17T10:58:02.714690Z","shell.execute_reply":"2024-06-17T10:58:02.713877Z","shell.execute_reply.started":"2024-06-17T10:58:02.708511Z"},"trusted":true},"outputs":[],"source":["label_counts={ \"living room\" : 0,\n","                    \"kitchen\" : 0 ,\n","                    \"bedroom\" : 0,\n","                    \"bathroom\":0,\n","                    \"dining room\" : 0}"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-17T13:24:02.929905Z","iopub.status.busy":"2024-06-17T13:24:02.929198Z","iopub.status.idle":"2024-06-17T13:24:10.244744Z","shell.execute_reply":"2024-06-17T13:24:10.243534Z","shell.execute_reply.started":"2024-06-17T13:24:02.929872Z"},"papermill":{"duration":14.934409,"end_time":"2024-04-26T06:29:19.995148","exception":false,"start_time":"2024-04-26T06:29:05.060739","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["import os\n","import json\n","import random\n","from PIL import Image\n","import torchvision.transforms as transforms\n","\n","# Load the JSON file containing image paths\n","json_file_path = \"/kaggle/input/bbbbbbbbbbbbbbbbbbbbbb/train.json\"\n","with open(json_file_path, 'r') as f:\n","    data = json.load(f)\n","\n","# Load all images into memory and preprocess them\n","transform = transforms.Compose([\n","    transforms.Resize(224),\n","    transforms.CenterCrop(224),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","])\n","\n","Train_images = []\n","Train_labels1 = []\n","testing_images = []\n","Testing_labels = []\n","\n","counts_test =0\n","\n","# Dictionary to keep track of image counts for each label\n","#label_counts_test = {\"Bathroom\": 0, \"Bedroom\": 0, \"Dining Room\": 0, \"Kitchen\": 0, \"Living Room\": 0}\n","\n","# Shuffle the data\n","random.shuffle(data)\n","\n","for image_info in data:\n","    image_path = os.path.join('/kaggle/input/data-set/data_set/', image_info['image_path'])\n","    t1 = image_info['label2'] \n","    image_path = image_path.replace('\\\\', '/')\n","    image_names.append(os.path.basename(image_path))\n","    if os.path.exists(image_path)and counts_test > 20000 and image_path not in image_names:\n","        image = Image.open(image_path).convert('RGB')\n","        image = transform(image)\n","        testing_images.append(image)\n","        Testing_labels.append(t1)\n","#         # Increment the count for the current label\n","        counts_test += 1\n","    \n","        \n","#     # Check if we have read 1000 images for each label\n","#     if all(count >= 500 for count in label_counts_test.values()):\n","#         break\n","\n","        \n","print(len(testing_images))\n","print(len(Testing_labels))"]},{"cell_type":"code","execution_count":125,"metadata":{"execution":{"iopub.execute_input":"2024-06-17T14:54:20.485854Z","iopub.status.busy":"2024-06-17T14:54:20.485260Z","iopub.status.idle":"2024-06-17T14:54:20.497428Z","shell.execute_reply":"2024-06-17T14:54:20.496242Z","shell.execute_reply.started":"2024-06-17T14:54:20.485804Z"},"trusted":true},"outputs":[{"data":{"text/plain":["9725"]},"execution_count":125,"metadata":{},"output_type":"execute_result"}],"source":["imhhh=image_names\n","len(imhhh)"]},{"cell_type":"code","execution_count":137,"metadata":{"execution":{"iopub.execute_input":"2024-06-17T15:04:05.261385Z","iopub.status.busy":"2024-06-17T15:04:05.260366Z","iopub.status.idle":"2024-06-17T15:04:05.267836Z","shell.execute_reply":"2024-06-17T15:04:05.266690Z","shell.execute_reply.started":"2024-06-17T15:04:05.261351Z"},"trusted":true},"outputs":[{"data":{"text/plain":["29727"]},"execution_count":137,"metadata":{},"output_type":"execute_result"}],"source":["len(image_names)"]},{"cell_type":"code","execution_count":136,"metadata":{"execution":{"iopub.execute_input":"2024-06-17T15:01:58.666416Z","iopub.status.busy":"2024-06-17T15:01:58.666011Z","iopub.status.idle":"2024-06-17T15:03:58.031106Z","shell.execute_reply":"2024-06-17T15:03:58.030062Z","shell.execute_reply.started":"2024-06-17T15:01:58.666365Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["10002\n","10002\n"]}],"source":["import os\n","import json\n","import random\n","from PIL import Image\n","import torchvision.transforms as transforms\n","\n","# Load the JSON file containing image paths\n","json_file_path = \"/kaggle/input/bbbbbbbbbbbbbbbbbbbbbb/train.json\"\n","with open(json_file_path, 'r') as f:\n","    data = json.load(f)\n","\n","# Load all images into memory and preprocess them\n","transform = transforms.Compose([\n","    transforms.Resize(224),\n","    transforms.CenterCrop(224),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","])\n","Train_images = []\n","Train_labels1 = []\n","testing_images = []\n","Testing_labels = []\n","imageee_names = []\n","counts_test =0\n","\n","# Dictionary to keep track of image counts for each label\n","#label_counts_test = {\"Bathroom\": 0, \"Bedroom\": 0, \"Dining Room\": 0, \"Kitchen\": 0, \"Living Room\": 0}\n","\n","# Shuffle the data\n","random.shuffle(data)\n","\n","for image_info in data:\n","    image_path = os.path.join('/kaggle/input/data-set/data_set/', image_info['image_path'])\n","    t1 = image_info['label2'] \n","    image_path = image_path.replace('\\\\', '/')\n","    imagggg=os.path.basename(image_path)\n","    if os.path.exists(image_path) and imagggg not in image_names:\n","        image_names.append(imagggg)\n","        imageee_names.append(imagggg)\n","        image = Image.open(image_path).convert('RGB')\n","        image = transform(image)\n","        testing_images.append(image)\n","        Testing_labels.append(t1)\n","        \n","#         # Increment the count for the current label\n","        counts_test += 1\n","        \n","#     # Check if we have read 1000 images for each label\n","#     if all(count >= 500 for count in label_counts_test.values()):\n","#         break\n","\n","        \n","print(len(testing_images))\n","print(len(Testing_labels))"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.007369,"end_time":"2024-04-26T06:29:20.010127","exception":false,"start_time":"2024-04-26T06:29:20.002758","status":"completed"},"tags":[]},"source":["# **encoding ad converting into tensors**"]},{"cell_type":"code","execution_count":32,"metadata":{"execution":{"iopub.execute_input":"2024-06-17T11:53:05.163580Z","iopub.status.busy":"2024-06-17T11:53:05.162912Z","iopub.status.idle":"2024-06-17T11:53:05.168256Z","shell.execute_reply":"2024-06-17T11:53:05.167178Z","shell.execute_reply.started":"2024-06-17T11:53:05.163549Z"},"trusted":true},"outputs":[],"source":["label_mapping={ \"living room\" : 0,\n","                    \"kitchen\" : 1 ,\n","                    \"bedroom\" : 2,\n","                    \"bathroom\":3,\n","                    \"dining room\" : 4}"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-06-17T10:59:43.918607Z","iopub.status.busy":"2024-06-17T10:59:43.918200Z","iopub.status.idle":"2024-06-17T10:59:44.393617Z","shell.execute_reply":"2024-06-17T10:59:44.392631Z","shell.execute_reply.started":"2024-06-17T10:59:43.918579Z"},"papermill":{"duration":0.960677,"end_time":"2024-04-26T06:29:20.978292","exception":false,"start_time":"2024-04-26T06:29:20.017615","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["import torch\n","from torch.utils.data import DataLoader, TensorDataset\n","from torchvision import transforms\n","from sklearn.preprocessing import LabelEncoder\n","def encode_labels(original_labels):\n","    # Define a dictionary mapping categories to numerical values\n","    #label_mapping = {\"Bathroom\": \"0\", \"Bedroom\": \"1\",\"Dining Room\": \"2\",\"Kitchen\": \"3\",\"Living Room\": \"4\"}\n","    \n","    # Initialize an empty list to store the encoded labels\n","    encoded_labels = []\n","    \n","    # Iterate through the original labels and encode each label\n","    for label in original_labels:\n","        encoded_labels.append(label_mapping[label])\n","    \n","    return encoded_labels\n","\n","\n"]},{"cell_type":"code","execution_count":45,"metadata":{"execution":{"iopub.execute_input":"2024-06-17T12:53:27.164931Z","iopub.status.busy":"2024-06-17T12:53:27.164225Z","iopub.status.idle":"2024-06-17T12:53:29.059885Z","shell.execute_reply":"2024-06-17T12:53:29.059059Z","shell.execute_reply.started":"2024-06-17T12:53:27.164897Z"},"papermill":{"duration":2.148503,"end_time":"2024-04-26T06:29:23.134572","exception":false,"start_time":"2024-04-26T06:29:20.986069","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["\n","# Convert lists to PyTorch tensors\n","images_tensor = torch.stack(Train_images)\n","\n","# Encode label1\n","\n","labels_encoded = encode_labels(Train_labels1)\n","\n","labels_tensor = torch.tensor([int(item) for item in labels_encoded])\n","\n","# Create a TensorDataset\n","dataset = TensorDataset(images_tensor, labels_tensor)\n","\n","# Create a DataLoader\n","batch_size = 32  \n","dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n","\n","dataset = []\n","Train_images = []"]},{"cell_type":"code","execution_count":46,"metadata":{"execution":{"iopub.execute_input":"2024-06-17T12:53:29.062370Z","iopub.status.busy":"2024-06-17T12:53:29.062061Z","iopub.status.idle":"2024-06-17T12:53:29.253326Z","shell.execute_reply":"2024-06-17T12:53:29.252376Z","shell.execute_reply.started":"2024-06-17T12:53:29.062337Z"},"papermill":{"duration":0.285384,"end_time":"2024-04-26T06:29:23.427834","exception":false,"start_time":"2024-04-26T06:29:23.142450","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["1963\n","795\n","829\n","1203\n","1580\n"]}],"source":["from collections import Counter\n","\n","# List of labels\n","ct1=0\n","ct2=0\n","ct3=0\n","ct4=0\n","ct5=0\n","\n","# Count the occurrences of each label\n","label_counts = Counter(labels_tensor)\n","\n","# Print the counts\n","for label in label_counts:\n","    if label == 0:\n","        ct1+=1\n","    if label == 1:\n","        ct2+=1\n","    if label == 2:\n","        ct3+=1\n","    if label == 3:\n","        ct4+=1\n","    if label == 4:\n","        ct5+=1\n","    \n","    \n","    \n","    \n","print(ct1)\n","print(ct2)\n","print(ct3)\n","print(ct4)\n","print(ct5)"]},{"cell_type":"code","execution_count":138,"metadata":{"execution":{"iopub.execute_input":"2024-06-17T15:04:35.285594Z","iopub.status.busy":"2024-06-17T15:04:35.285193Z","iopub.status.idle":"2024-06-17T15:04:37.569585Z","shell.execute_reply":"2024-06-17T15:04:37.568579Z","shell.execute_reply.started":"2024-06-17T15:04:35.285562Z"},"papermill":{"duration":0.080724,"end_time":"2024-04-26T06:29:23.516356","exception":false,"start_time":"2024-04-26T06:29:23.435632","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["10002\n","hi\n"]}],"source":["images_tensor = torch.stack(testing_images)\n","\n","# Encode label1\n","\n","labels_encoded_test = encode_labels(Testing_labels)\n","\n","labels_tensor = torch.tensor([int(item) for item in labels_encoded_test])\n","print(len(labels_tensor))\n","#labels_tensor = torch.tensor(labels_encoded_test)\n","print(\"hi\")\n","# Create a TensorDataset\n","dataset = TensorDataset(images_tensor, labels_tensor)\n","\n","# Create a DataLoader\n","batch_size = 32  \n","dataloadertest = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n","\n","\n","dataset= []\n","testing_images = []"]},{"cell_type":"code","execution_count":139,"metadata":{"execution":{"iopub.execute_input":"2024-06-17T15:04:39.093220Z","iopub.status.busy":"2024-06-17T15:04:39.092339Z","iopub.status.idle":"2024-06-17T15:04:39.373475Z","shell.execute_reply":"2024-06-17T15:04:39.372443Z","shell.execute_reply.started":"2024-06-17T15:04:39.093188Z"},"papermill":{"duration":0.047199,"end_time":"2024-04-26T06:29:23.571580","exception":false,"start_time":"2024-04-26T06:29:23.524381","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["3118\n","1167\n","1325\n","1930\n","2462\n"]}],"source":["from collections import Counter\n","\n","# List of labels\n","ct1=0\n","ct2=0\n","ct3=0\n","ct4=0\n","ct5=0\n","\n","# Count the occurrences of each label\n","label_counts = Counter(labels_tensor)\n","\n","# Print the counts\n","for label in label_counts:\n","    if label == 0:\n","        ct1+=1\n","    if label == 1:\n","        ct2+=1\n","    if label == 2:\n","        ct3+=1\n","    if label == 3:\n","        ct4+=1\n","    if label == 4:\n","        ct5+=1\n","    \n","    \n","    \n","    \n","print(ct1)\n","print(ct2)\n","print(ct3)\n","print(ct4)\n","print(ct5)"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.00769,"end_time":"2024-04-26T06:29:23.587058","exception":false,"start_time":"2024-04-26T06:29:23.579368","status":"completed"},"tags":[]},"source":["# Model"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-06-17T10:59:58.518290Z","iopub.status.busy":"2024-06-17T10:59:58.517660Z","iopub.status.idle":"2024-06-17T10:59:58.526614Z","shell.execute_reply":"2024-06-17T10:59:58.525583Z","shell.execute_reply.started":"2024-06-17T10:59:58.518257Z"},"papermill":{"duration":0.018889,"end_time":"2024-04-26T06:29:23.614042","exception":false,"start_time":"2024-04-26T06:29:23.595153","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torchvision import models\n","\n","# Function to extract features and compute accuracy\n","def extract_features_and_compute_accuracy(model, dataloader):\n","    correct = 0\n","    total = 0\n","    features = []\n","    labels = []\n","    with torch.no_grad():\n","        for inputs, targets in dataloader:\n","            inputs = inputs.to(device)\n","            outputs = model.backbone(inputs)\n","            features.append(outputs.cpu().numpy())\n","            labels.append(targets.cpu().numpy())\n","            outputs = model(inputs)\n","            _, predicted = torch.max(outputs, 1)\n","            total += targets.size(0)\n","            #print(predicted)\n","            correct += (predicted == targets.to(device)).sum().item()\n","    features = np.concatenate(features, axis=0)\n","    labels = np.concatenate(labels, axis=0)\n","    accuracy = correct / total\n","    return accuracy, features, labels\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["!smi-nvidia\n"]},{"cell_type":"code","execution_count":47,"metadata":{"execution":{"iopub.execute_input":"2024-06-17T12:53:42.799431Z","iopub.status.busy":"2024-06-17T12:53:42.798740Z","iopub.status.idle":"2024-06-17T13:10:37.686408Z","shell.execute_reply":"2024-06-17T13:10:37.685334Z","shell.execute_reply.started":"2024-06-17T12:53:42.799365Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch [61/80], Loss: 0.6431, Accuracy: 0.7956\n","Epoch [62/80], Loss: 0.3407, Accuracy: 0.8856\n","Epoch [63/80], Loss: 0.1830, Accuracy: 0.9380\n","Epoch [64/80], Loss: 0.0930, Accuracy: 0.9705\n","Epoch [65/80], Loss: 0.0512, Accuracy: 0.9863\n","Epoch [66/80], Loss: 0.0365, Accuracy: 0.9909\n","Epoch [67/80], Loss: 0.0220, Accuracy: 0.9943\n","Epoch [68/80], Loss: 0.0181, Accuracy: 0.9947\n","Epoch [69/80], Loss: 0.0201, Accuracy: 0.9937\n","Epoch [70/80], Loss: 0.0135, Accuracy: 0.9956\n","Epoch [71/80], Loss: 0.0101, Accuracy: 0.9964\n","Epoch [72/80], Loss: 0.0117, Accuracy: 0.9958\n","Epoch [73/80], Loss: 0.0076, Accuracy: 0.9973\n","Epoch [74/80], Loss: 0.0132, Accuracy: 0.9948\n","Epoch [75/80], Loss: 0.0104, Accuracy: 0.9959\n","Epoch [76/80], Loss: 0.0163, Accuracy: 0.9940\n","Epoch [77/80], Loss: 0.0092, Accuracy: 0.9967\n","Epoch [78/80], Loss: 0.0062, Accuracy: 0.9980\n","Epoch [79/80], Loss: 0.0052, Accuracy: 0.9980\n","Epoch [80/80], Loss: 0.0055, Accuracy: 0.9976\n"]}],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torchvision import models\n","\n","# Define your BenchmarkModel\n","class BenchmarkModel(nn.Module):\n","    def __init__(self, backbone, num_classes):\n","        super(BenchmarkModel, self).__init__()\n","        self.backbone = backbone\n","        # Replace the classifier of VGG16\n","        in_features = self.backbone.classifier[6].in_features\n","        self.backbone.classifier[6] = nn.Linear(in_features, 1024)\n","        \n","    def forward(self, x):\n","        x = self.backbone(x)\n","        return x\n","\n","# Load pre-trained VGG16 model\n","pre_trained = models.vgg16(pretrained=True)\n","\n","for param in pre_trained.features[:-12].parameters():\n","    param.requires_grad = True\n","\n","# Replace the last layer with the BenchmarkModel\n","num_classes = 1024  # As you want the output to be 1024-dimensional\n","The_model = BenchmarkModel(pre_trained, num_classes)\n","\n","# Move model to GPU if available\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","The_model = The_model.to(device)\n","\n","# Define loss function and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.SGD(The_model.parameters(), lr=0.001, momentum=0.9)\n","\n","# Load checkpoint\n","checkpoint = torch.load(\"/kaggle/working/checkpoint.pth\")\n","The_model.load_state_dict(checkpoint['model_state_dict'])\n","optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","start_epoch = checkpoint['epoch'] + 1\n","\n","# Uncomment the following code if you want to continue training from the checkpoint\n","num_epochs = 80  # Adjust as needed\n","for epoch in range(start_epoch, start_epoch+20):\n","    The_model.train()\n","    running_loss = 0.0\n","    correct_predictions = 0\n","    total_predictions = 0\n","    for inputs, targets in dataloader:\n","        inputs, targets = inputs.to(device), targets.to(device)\n","        optimizer.zero_grad()\n","        outputs = The_model(inputs)\n","        loss = criterion(outputs, targets)\n","        loss.backward()\n","        optimizer.step()\n","        running_loss += loss.item() * inputs.size(0)\n","        \n","        _, predicted = torch.max(outputs, 1)\n","        correct_predictions += torch.sum(predicted == targets).item()\n","        total_predictions += targets.size(0)\n","    \n","    epoch_loss = running_loss / len(dataloader.dataset)\n","    epoch_accuracy = correct_predictions / total_predictions\n","    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.4f}\")\n","\n","    # Save the checkpoint after each epoch\n","torch.save({\n","        'epoch': epoch,\n","        'model_state_dict': The_model.state_dict(),\n","        'optimizer_state_dict': optimizer.state_dict(),\n","        'loss': epoch_loss,\n","}, \"checkpoint.pth\")\n","\n","# Save the fine-tuned model\n","\n","torch.save(The_model.state_dict(), \"fine_tuned_vgg16.pth\")\n"]},{"cell_type":"code","execution_count":140,"metadata":{"execution":{"iopub.execute_input":"2024-06-17T15:05:09.158941Z","iopub.status.busy":"2024-06-17T15:05:09.157995Z","iopub.status.idle":"2024-06-17T15:06:02.297909Z","shell.execute_reply":"2024-06-17T15:06:02.296927Z","shell.execute_reply.started":"2024-06-17T15:05:09.158908Z"},"papermill":{"duration":6.282868,"end_time":"2024-04-26T06:56:05.681150","exception":false,"start_time":"2024-04-26T06:55:59.398282","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Fine-tuned VGG16 Testing Accuracy: 0.9293141371725655\n"]}],"source":["# Evaluate the model\n","import pickle\n","The_model.eval()\n","accuracy, features, labels = extract_features_and_compute_accuracy(The_model, dataloadertest)\n","print(f\"Fine-tuned VGG16 Testing Accuracy: {accuracy}\")\n","#features_labels = np.column_stack((features, labels.reshape(-1, 1)))\n","image_features_dict = dict(zip(imageee_names, features))\n","#print(image_features_dict)\n","\n","\n","# Save features and labels for VGG16 in a .pkl file\n","with open(\"vgg16_features_labels_trainppp11_1024.pkl\", \"wb\") as f:\n","    pickle.dump(image_features_dict, f)\n","\n"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4856438,"sourceId":8198346,"sourceType":"datasetVersion"},{"datasetId":4862172,"sourceId":8704241,"sourceType":"datasetVersion"},{"sourceId":183732203,"sourceType":"kernelVersion"}],"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"papermill":{"default_parameters":{},"duration":6285.831299,"end_time":"2024-04-26T08:12:15.676053","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-04-26T06:27:29.844754","version":"2.5.0"}},"nbformat":4,"nbformat_minor":4}
